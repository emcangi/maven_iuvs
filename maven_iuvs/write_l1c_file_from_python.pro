pro write_l1c_file_from_python, l1a_filepath, l1a_dark_filepath, saveloc, brightness_csv, ph_per_s_csv
  ; ----------------------------------------------------------------------------------------------------------------------------
  ;  Eryn Cangi, CU Boulder, Laboratory for Atmospheric and Space Physics, March 2024
  ;  Updated: 2025
  ;
  ;   Writes out an l1c IUVS echelle file and associated .xml label. This code has been adapted from the IDL pipeline to accept
  ;   input generated by the new Python pipeline. In future, this will be adapted so that Python is able to write out the files.
  ;   This version properly tracks the dark filename and writes that out to the header as well. 
  ;   
  ; ------------------------------------------------------------------------------------------------------------------------
  ;
  ; inputs:
  ;   - l1a_filename: string specifying the original l1a filename (no folder info).
  ;   - l1c_filename: string specifying the name of the new l1c file (no folder info). 
  ;   - emissions: a dictionary object containing some labels of the emission names. TODO: May be generated in this script.
  ;   - brightnesses: Dictionary object containing the captured brightnesses of the H and D Ly alpha emissions. Shape (ni,), 
  ;                   where ni is the number of integrations in the parent file. one entry for each integration in the file.
  ;   - observation: Dictionary object containing other data that will be saved to the l1c.
  ;   - saveloc: folder path to store the l1c file.
  ;
  ; outputs:
  ;   - a level1c fits file
  ;
  ; functions called:
  ;   - 
  ; ------------------------------------------------------------------------------------------------------------------------
  
  ; Load L1a =====================================================================================
  l1a_fits = iuvs_read_fits(l1a_filepath);  + l1a_filename)
  Nwaves  = N_elements(l1a_fits.primary(*,0,0))
  Nspaces = N_elements(l1a_fits.primary(0,*,0))
  Nframes = N_elements(l1a_fits.primary(0,0,*))
  
  ; Filenames ====================================================================================
  l1asplit = l1a_filepath.split("/")
  l1a_filename = l1asplit[n_elements(l1asplit)-1]
  l1adarksplit = l1a_dark_filepath.split("/")  ; Need to distinguish the dark filename so it can be written out
  l1a_dark_filename = l1adarksplit[n_elements(l1adarksplit)-1] ; Need to distinguish the dark filename so it can be written out
  l1c_filename = StrJoin( StrSplit(l1a_filename, '_l1._', /regex, /extract), '_l1c_')
  l1c_filename = l1c_filename.replace('v13', 'v15')
  l1c_filename = l1c_filename.replace('v14', 'v15')
  print, "IDL will write out to: " + l1c_filename
  
  ; LOAD STUFF FROM PYTHON =======================================================================
  ; !!!!!!!!!!!!!!!!!!!!
  ; !!!!! TO DO !!!!!!!!
  ; !!!!!!!!!!!!!!!!!!!!
  ; THIS IS JUST ONE SET OF CSVs! eVENTAULLY THIS NEEDS TO ACCEPT EACH CSV FOR EACH FILE!
  ; Load files that were written out 
  brightness_data = READ_CSV(brightness_csv, HEADER=BrightHeader, N_TABLE_HEADER=0)
 
  ; fill a struct using the column names so that we can actually index by name, I can't believe this isn't built in.
  st = CREATE_STRUCT( BrightHeader[0], brightness_data.(0) )
  FOR i = 1, N_ELEMENTS(BrightHeader) - 1 DO begin
    st = CREATE_STRUCT( st, BrightHeader[i], brightness_data.(i) )
  ENDFOR 

  ; fill the BRIGHT_DATA_Ph_per_s array from the file
  ph_per_s_data = READ_CSV(ph_per_s_csv, HEADER=PhPerSHeader, N_TABLE_HEADER=0)

  ; add in the entries we couldn't get from Python because lazy
  rows = n_elements(PhPerSHeader)
  cols = n_elements(l1a_fits.OBSERVATION.WAVELENGTH[*, 0])
  ph_per_s_arr = DBLARR(rows, cols)
  
  for i = 0, rows-1 do begin
    ph_per_s_arr[i, *] = ph_per_s_data.(i) ;
  endfor
  st = CREATE_STRUCT(st, "BRIGHT_DATA_Ph_per_sec", ph_per_s_arr)
  
  ; CARRY FORWARD L1A HDUS =======================================================================
  
  L1A_PRIMARY = l1a_fits.PRIMARY ; Will be written out as a simple array

  ; INTEGRATION HDU ------------------------------------------------------------------------------
  INTEGRATION_TEMPLATE = {$
    TIMESTAMP:0.0d,$
    ET:0.0d,$
    UTC:'',$
    MIRROR_DN:0L,$
    MIRROR_DEG:0.0d,$
    FOV_DEG:0.0d,$
    LYA_CENTROID:0L,$
    CASE_TEMP: 0L,$
    DET_TEMP: 0L$
  }
  INTEGRATION = MAKE_ARRAY(Nframes, VALUE=INTEGRATION_TEMPLATE)
  STRUCT_ASSIGN, l1a_fits.INTEGRATION, INTEGRATION
  
  ; ENGINEERING HDU ------------------------------------------------------------------------------
  ENGINEERING_TEMPLATE = {$
    SCI_PKT_CKSUM: 0L,$
    SCI_ERR_FLAGS: 0L,$
    XUV:'',$
    LENGTH: 0L,$
    IMAGE_NUMBER: 0L,$
    AVERAGE: 0L,$
    CHECKSUM: 0L,$
    START_TIME: 0L,$
    START_TIME__SUB: 0L,$
    CADENCE: 0L,$
    NUMBER: 0L,$
    INT_TIME: 0L,$
    MIRROR_POS: 0L,$
    STEP_NUM: 0L,$
    STEP_SIZE: 0L,$
    STEP_INT: 0L,$
    BIN_SHIFT: 0L,$
    OBS_ID: 0L,$
    FUV_BAD_PIXEL_MASK: 0L,$
    MUV_BAD_PIXEL_MASK: 0L,$
    DATA_COMPRESSION: 0L,$
    TEST_PATTERN: 0L,$
    ON_CHIP_WINDOWING: 0L,$
    BIN_TYPE:'',$
    SCAN_MODE: '',$
    MODE: '',$
    TIME_FLAG: '',$
    BIN_SHIFT_DIR: 0L,$
    SHUTTER_ON: 0L,$
    SHUTTER_OFF: 0L,$
    SHUTTER_NUM: 0L,$
    SET_TOTAL: 0L,$
    BIN_X_ROW: 0L,$
    BIN_Y_ROW: 0L,$
    MCP_GAIN: 0L,$
    SCI_SEG_TOTAL: 0L,$
    SCI_SEG_LENGTH: 0L,$
    SCI_SEG_NUM: 0L,$
    PROCESS_DATE: '',$
    SCI_IMG_DATA_LENGTH_: 0L$
  }
  ENGINEERING = MAKE_ARRAY(1, VALUE=ENGINEERING_TEMPLATE)
  STRUCT_ASSIGN, l1a_fits.ENGINEERING, ENGINEERING
    
  ; BINNING HDU ------------------------------------------------------------------------------
  BINNING_TEMPLATE = {$
    SPABINWIDTH: MAKE_ARRAY(Nspaces+2, /INTEGER),$
    SPABINTRANSMIT: MAKE_ARRAY(Nspaces+2, /INTEGER),$
    SPEBINWIDTH: MAKE_ARRAY(Nwaves+2, /INTEGER),$
    SPEBINTRANSMIT: MAKE_ARRAY(Nwaves+2, /INTEGER),$
    SPAPIXLO: MAKE_ARRAY(Nspaces, /INTEGER),$
    SPAPIXHI: MAKE_ARRAY(Nspaces, /INTEGER),$
    SPEPIXLO: MAKE_ARRAY(Nwaves, /INTEGER),$
    SPEPIXHI: MAKE_ARRAY(Nwaves, /INTEGER),$
    BINTABLENAME: '' $
  }
  BINNING = MAKE_ARRAY(1, VALUE=BINNING_TEMPLATE)
  STRUCT_ASSIGN, l1a_fits.BINNING, BINNING

  ; PIXELGEOMETRY HDU ------------------------------------------------------------------------------
  PIXELGEOMETRY_TEMPLATE = {$
    PIXEL_VEC: MAKE_ARRAY(5, 74, 3, /DOUBLE), $
    PIXEL_CORNER_RA: DBLARR(5, 74), $
    PIXEL_CORNER_DEC: DBLARR(5, 74), $
    PIXEL_CORNER_LAT: DBLARR(5, 74), $
    PIXEL_CORNER_LON: DBLARR(5, 74), $
    PIXEL_CORNER_MRH_ALT: DBLARR(5, 74), $
    PIXEL_CORNER_MRH_ALT_RATE: DBLARR(5, 74), $
    PIXEL_CORNER_LOS: DBLARR(5, 74), $
    PIXEL_SOLAR_ZENITH_ANGLE: DBLARR(74), $
    PIXEL_EMISSION_ANGLE: DBLARR(74), $
    PIXEL_ZENITH_ANGLE: DBLARR(74), $
    PIXEL_PHASE_ANGLE: DBLARR(74), $
    PIXEL_LOCAL_TIME: DBLARR(74), $
    PIXEL_CORNER_SOLAR_ZENITH_ANGLE: DBLARR(5, 74), $
    PIXEL_CORNER_EMISSION_ANGLE: DBLARR(5, 74), $
    PIXEL_CORNER_ZENITH_ANGLE: DBLARR(5, 74), $
    PIXEL_CORNER_PHASE_ANGLE: DBLARR(5, 74), $
    PIXEL_CORNER_LOCAL_TIME: DBLARR(5, 74) $
  }
  PIXELGEOMETRY = MAKE_ARRAY(Nframes, VALUE=PIXELGEOMETRY_TEMPLATE)
  STRUCT_ASSIGN, l1a_fits.PIXELGEOMETRY, PIXELGEOMETRY
  
  ; SPACECRAFTGEOMETRY HDU ------------------------------------------------------------------------------
  SPACECRAFTGEOMETRY_TEMPLATE = {$
    SUB_SPACECRAFT_LAT: 0.0d, $
    SUB_SPACECRAFT_LON: 0.0d, $
    SUB_SOLAR_LAT: 0.0d, $
    SUB_SOLAR_LON: 0.0d, $
    SPACECRAFT_ALT: 0.0d, $
    V_SPACECRAFT: DBLARR(3), $
    V_SPACECRAFT_RATE: DBLARR(3), $
    V_SUN: DBLARR(3), $
    V_SUN_RATE: DBLARR(3), $
    VX_SPACECRAFT: DBLARR(3), $
    VY_SPACECRAFT: DBLARR(3), $
    VZ_SPACECRAFT: DBLARR(3), $
    VX_INSTRUMENT: DBLARR(3), $
    VY_INSTRUMENT: DBLARR(3), $
    VZ_INSTRUMENT: DBLARR(3), $
    V_SPACECRAFT_MSO: DBLARR(3), $
    V_SPACECRAFT_RATE_MSO: DBLARR(3), $
    V_SPACECRAFT_INERTIAL: DBLARR(3), $
    V_SPACECRAFT_RATE_INERTIAL: DBLARR(3), $
    V_SPACECRAFT_INERTIAL_FRAME: '', $
    V_SPACECRAFT_INERTIAL_CENTER: '', $
    V_SUN_MSO: DBLARR(3),$
    V_SUN_RATE_MSO: DBLARR(3),$
    VX_SPACECRAFT_MSO: DBLARR(3),$
    VY_SPACECRAFT_MSO: DBLARR(3),$
    VZ_SPACECRAFT_MSO: DBLARR(3),$
    VX_INSTRUMENT_MSO: DBLARR(3),$
    VY_INSTRUMENT_MSO: DBLARR(3),$
    VZ_INSTRUMENT_MSO: DBLARR(3),$
    VX_SPACECRAFT_INERTIAL: DBLARR(3),$
    VY_SPACECRAFT_INERTIAL: DBLARR(3),$
    VZ_SPACECRAFT_INERTIAL: DBLARR(3),$
    VX_INSTRUMENT_INERTIAL: DBLARR(3),$
    VY_INSTRUMENT_INERTIAL: DBLARR(3),$
    VZ_INSTRUMENT_INERTIAL: DBLARR(3),$
    INST_SUN_ANGLE: 0.0d $
  }
  SPACECRAFTGEOMETRY = MAKE_ARRAY(Nframes, VALUE=SPACECRAFTGEOMETRY_TEMPLATE)
  STRUCT_ASSIGN, l1a_fits.SPACECRAFTGEOMETRY, SPACECRAFTGEOMETRY
  

  ; EMISSIONS HDU -----------------------------------------------------------------------------------
  EMISSIONS = make_array(2, value={SPECIES_ID:''}) ; 2 for 2 species.
  EMISSIONS.SPECIES_ID = ['H_1215.67_Angstrom', 'D_1215.34_Angstrom']        ; emission feature names
  
  ; BRIGHTNESS HDU ----------------------------------------------------------------------------------

  BRIGHTNESSES_TEMPLATE = {$
    BRIGHT_H_kR:0.0d,$                                           ; to populate with H brightness (BkR_H) in kR - FROM PYTHON
    BRIGHT_D_kR:0.0d,$                                           ; to populate with D brightness (BkR_D) in kR - FROM PYTHON
    BRIGHT_DATA_Ph_per_sec:DBLARR(cols),$               ; to populate with H_SPEC_ABV_BCK in Ph/s - FROM PYTHON
    BRIGHT_H_OneSIGMA_kR:0.0d,$                                    ; Fit uncertainty for H (BkR_U) in kR - FROM PYTHON
    BRIGHT_D_OneSIGMA_kR:0.0d,$                                    ; Fit uncertainty for D (BkR_U) in kR - FROM PYTHON
    MRH_ALTITUDE_km:0.0d,$                                       ; to populate with MRH in km - FROM PYTHON
    TANGENT_SZA_deg:0.0d,$                                       ; to populate with SZA in degrees - FROM PYTHON
    ET:0.0d,$                                                      ; Ephemeris time - FROM PYTHON
    LYMAN_ALPHA_WAVELENGTH_Angstrom:DBLARR(cols),$      ; to populate with the H & D wavelength array, H_WL, in Angstrom - FROM IDL (L1a)
    UTC:''$                                                      ; UTC time of observation - FROM PYTHON
  }
 
  ; populate the structure
  BRIGHTNESSES=MAKE_ARRAY(rows, VALUE=BRIGHTNESSES_TEMPLATE)
  STRUCT_ASSIGN, l1a_fits.SPACECRAFTGEOMETRY, BRIGHTNESSES
  BRIGHTNESSES.ET = l1a_fits.INTEGRATION.ET
  BRIGHTNESSES.UTC = l1a_fits.INTEGRATION.UTC
  BRIGHTNESSES.BRIGHT_H_kR = st.BRIGHT_H_KR
  BRIGHTNESSES.BRIGHT_D_kR = st.BRIGHT_D_KR
  BRIGHTNESSES.BRIGHT_DATA_Ph_per_sec = transpose(st.BRIGHT_DATA_Ph_per_sec) ; Transform the shape because astropy.io.fits uses a different ordering.
  ; Problem: The original pipeline was only writing out the final spectrum from the last integration 10 times
  ; .... so that was wrong...
  BRIGHTNESSES.BRIGHT_H_OneSIGMA_kR = st.BRIGHT_H_OneSIGMA_kR
  BRIGHTNESSES.BRIGHT_D_OneSIGMA_kR = st.BRIGHT_D_OneSIGMA_kR
  BRIGHTNESSES.MRH_ALTITUDE_km = st.MRH_ALTITUDE_KM
  BRIGHTNESSES.TANGENT_SZA_deg = st.TANGENT_SZA_DEG
  BRIGHTNESSES.LYMAN_ALPHA_WAVELENGTH_Angstrom = l1a_fits.OBSERVATION.WAVELENGTH[*, 0] * 10 ; convert to angstrom; file stores nm
  ; The IDL pipeline constructs a wavelength array that we didn't use with Python.
  
  ; OBSERVATION HDU---------------------------------------------------------------------------------------------------
  OBSERVATION_TEMPLATE = {$
    PRODUCT_ID:  '',$
    COLLECTION_ID:  '',$
    BUNDLE_ID:  '',$
    SOFTWARE_VERSION: '',$
    CODE_SVN_REVISION: '',$
    ANC_SVN_REVISION: '',$
    PRODUCT_CREATION_DATE:  '',$
    OBSERVATION_TYPE:  '',$
    MISSION_PHASE: '',$
    TARGET_NAME: '',$
    ORBIT_SEGMENT: '',$
    ORBIT_NUMBER: 0L,$
    SOLAR_LONGITUDE: 0.0d,$
    GRATING_SELECT:  '',$
    KEYHOLE_SELECT:  '',$
    BIN_PATTERN_INDEX: '',$
    CADENCE:  0.0d,$
    INT_TIME:  0.0d,$
    DUTY_CYCLE:  0.0d,$
    CHANNEL: '',$
    WAVELENGTH: DBLARR(Nwaves, Nspaces),$
    WAVELENGTH_WIDTH: DBLARR(Nwaves, Nspaces),$
    ; Figure out how many kernels will get printed in OBSERVATION.KERNELS so we can size it correctly --EMC 2/21/24
    KERNELS: STRARR(n_elements(reform(l1a_fits.OBSERVATION.KERNELS)) )$
  }
  OBSERVATION = MAKE_ARRAY(1, VALUE=OBSERVATION_TEMPLATE)
  STRUCT_ASSIGN, l1a_fits.OBSERVATION, OBSERVATION
  ;replace fields in the observation struct
  str_element, OBSERVATION, "PRODUCT_ID", l1c_filename.substring(0, -9), /ADD_REPLACE ; FROM IDL - done; don't include .fits.gz
  str_element, OBSERVATION, "BUNDLE_ID", "processed", /ADD_REPLACE
  str_element, OBSERVATION, "COLLECTION_ID", STRUPCASE(l1a_fits.OBSERVATION.COLLECTION_ID), /ADD_REPLACE
  str_element, OBSERVATION, "SOFTWARE_VERSION", strtrim(string(get_svn_revision()),2), /ADD_REPLACE ; TODO: get form Python
  str_element, OBSERVATION, "PRODUCT_CREATION_DATE", et_timout(sys_et()), /ADD_REPLACE ; TODO: get from Python
  str_element, OBSERVATION, "ORBIT_SEGMENT", st.ORBIT_SEGMENT[0], /ADD_REPLACE ; from Python because IDL regex is not as useful
  str_element, OBSERVATION, "ORBIT_NUMBER", fix(strmid(l1a_filename, strpos(l1a_filename, 'orbit')+5, 5)), /ADD_REPLACE
  ;str_element, KERNELS, reform(l1a_fits.OBSERVATION.KERNELS), /ADD_REPLACE
  str_element, OBSERVATION, "PRODUCT_CREATION_DATE", st.PRODUCT_CREATION_DATE[0], /ADD_REPLACE ; got from Python because that's what processed it
  

  ;STRUCT_ASSIGN, l1a_fits.OBSERVATION, OBSERVATION
  ;OBSERVATION.PRODUCT_ID      = l1c_filename.substring(0, -9) ; FROM IDL - done; don't include .fits.gz
  ;OBSERVATION.COLLECTION_ID   = 'ECHELLE'  ; FROM IDL  - done
  ;OBSERVATION.BUNDLE_ID       = 'REDUCED'  ; FROM IDL  - done
  ;OBSERVATION.SOFTWARE_VERSION= strtrim(string(get_svn_revision()),2)  ; FROM IDL - done?
  ;OBSERVATION.PRODUCT_CREATION_DATE= et_timout(sys_et()) ; TODO: load FROM PYTHON
  ;OBSERVATION.ORBIT_SEGMENT   = st.ORBIT_SEGMENT[0]  ; FROM PYTHON because idl doesn't seem to support lookarounds in regex.
  ;OBSERVATION.ORBIT_NUMBER    = fix(strmid(l1a_filename, strpos(l1a_filename, 'orbit')+5, 5))  ; FROM IDL - done
  ;OBSERVATION.KERNELS         = reform(l1a_fits.OBSERVATION.KERNELS)  ; FROM IDL - done 

  
  ; specify where to write the FITS file
  output_path = saveloc + l1c_filename ;+ '.fits.gz'
  clbl=1      ; write a label file
  
  iuvs_log,'Writing L1C echelle file'
  cspice_timout,l1a_fits.integration[0].et,'YYYY-MM-DDTHR:MN:SC.##Z ::UTC',25,start_time
  cspice_timout,l1a_fits.integration[-1].et+l1a_fits.engineering.int_time/1000d,'YYYY-MM-DDTHR:MN:SC.##Z ::UTC',25,stop_time
  fileinfo={ $
    start_time:start_time, $;Start of first integration in file in UTC
    stop_time: stop_time, $; End of last integration in file
    level_id: '1C',  $; PDS term: Use 'Raw' (1A) , 'Calibrated' (1B), 'Reduced' (upper level 1 (if any)), 'Derived' (any level 2)
    collection_id: STRLOWCASE(OBSERVATION.COLLECTION_ID) $
  }
  
  ; The corona L1C product has no image data, so set primary=0
  PRIMARY=0
 
  ; following code fixes a problem where the product names are cut off due to a limitation on
  ; number of characters in fits headers imposed by IDL fits writing routines
  ; 
  ; We need to be able to add a number of rows to the primary header that isn't fixed, but can be figured out.
  ; this because the L1BFILENAME and L1AFILENAME rows require special handling, as they are often over 80 characters,
  ; but the FITS writing software MWRFITS() truncates to 80. l1b can be handled by just shortening the message, because it
  ; doesn't need to include the l1a filename, but l1afilename obviously does.
  l1afn_msg = l1a_filename ; usually just be over 80 char
  l1astrstart = "L1AFILENAME = "

  ; max length we can use for a substring; this is the same for l1a or l1b.
  lensub = 80 - strlen(l1astrstart)

  ; number of rows to make to print the info
  l1afn_rows = ceil(float(strlen(l1afn_msg)) / float(lensub))
  l1amsg = strarr(l1afn_rows)

  j = 0 ; counter for where to start in the string
  for i=0, l1afn_rows-1 do begin
    l1amsg[i] = l1astrstart + l1afn_msg.substring(j, j+lensub-1)
    j += lensub
  endfor
  
  ; Now do the same thing for the dark file name, which we need to also include:
  l1adarkfn_msg = l1a_dark_filename ; this is the basic dark filename
  l1adarkstrstart = "L1ADARKFILE = "
  lensub_dark = 80 - strlen(l1adarkstrstart)
  l1adarkfn_rows = ceil(float(strlen(l1adarkfn_msg)) / float(lensub_dark))
  l1adarkmsg = strarr(l1adarkfn_rows)
  j = 0 ; counter for where to start in the string
  for i=0, l1adarkfn_rows-1 do begin
    l1adarkmsg[i] = l1adarkstrstart + l1adarkfn_msg.substring(j, j+lensub_dark-1)
    j += lensub_dark
  endfor
  
  ; Finally do the same thing for the l1c filename.  
  ; NOTE: This is special for this writeout file. This was not a problem in the FMR.
  l1cfn_msg = l1c_filename ; this is the basic dark filename
  l1cstrstart = "FILENAME = "
  lensub_l1cfn = 80 - strlen(l1cstrstart)
  l1cfn_rows = ceil(float(strlen(l1cfn_msg)) / float(lensub_l1cfn))
  l1cmsg = strarr(l1cfn_rows)

  j = 0 ; counter for where to start in the string
  for i=0, l1cfn_rows-1 do begin
    l1cmsg[i] = l1cstrstart + l1cfn_msg.substring(j, j+lensub_l1cfn-1)
    j += lensub_l1cfn
  endfor
  

  ; START NEW PRIMARY HEADER ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  n = n_elements(l1a_fits.PRIMARY_HEADER)
  addl_rows = 4; number of add'l rows for l1c: 2 extra comment rows, L1BFILENAME, SW_VER
  n_rows_total = n + l1afn_rows + l1adarkfn_rows + l1cfn_rows + addl_rows
  ph_orig = l1a_fits.PRIMARY_HEADER
  PRIMARY_HEADER = strarr(n_rows_total) ; l1afn_rows takes care of rows needed for L1AFILENAME and dark

  j = 0 ; next row to fill in PRIMARY_HEADER
  for i=0, n-2 do begin ;loop over the original header
    to_fill = ph_orig[i] ; initialize so we can write over it if need be

    ; change 1A to 1C
    if ph_orig[i].contains("COMMENT MAVEN IUVS Level 1A Data Product") then begin
      to_fill = "COMMENT MAVEN IUVS Level 1C Data Product"
    endif

    ; fill in the new comment rows before line starting with "BLANK"
    if ph_orig[i].contains("BLANK") then begin
      PRIMARY_HEADER[j] = "COMMENT File contains fluxes for these "+strtrim(string(n_elements(EMISSIONS.SPECIES_ID)),1)+" emission features:"
      PRIMARY_HEADER[j+1] = "COMMENT " + strjoin(EMISSIONS.SPECIES_ID, ' ')
      to_fill = ph_orig[i]
      j += 2
    endif
    
    ; fill the l1c filename. NOTE: This is special for this writeout file. This was not a problem in the FMR.
    if ph_orig[i].contains("FILENAME") then begin
      to_fill = l1cmsg[0]
      if n_elements(l1cmsg) gt 1 then begin
        for k=1, l1cfn_rows-1 do begin
          PRIMARY_HEADER[j+k] = l1cmsg[k]
        endfor
        j += l1cfn_rows
      endif
    endif

    ; to fill in the l1b, l1a names AFTER l1c filename.
    if ph_orig[i].contains("CAPTURE") then begin 

      PRIMARY_HEADER[j] = "L1BFILENAME = Echelle L1C was generated from L1A (see below) directly"
      ; fill the l1a filename
      for k=0, l1afn_rows-1 do begin
        PRIMARY_HEADER[j+1+k] = l1amsg[k]
      endfor
      j += (1 + l1afn_rows) ;  1 accounts for the L1B row, l1afn_rows for l1afn.
      
      ; fill the dark l1a filename
      for k=0, l1adarkfn_rows-1 do begin
        PRIMARY_HEADER[j+k] = l1adarkmsg[k]
      endfor
      
      to_fill = ph_orig[i]
      j += l1adarkfn_rows ; don't need to add another 1 yet 
    endif

    ; Update the last comment line to "processed" data
    if ph_orig[i].contains("COMMENT Raw") then begin
      to_fill = "COMMENT Processed data for this observation"
    endif

    ; finally, fill whatever is in to_fill.
    PRIMARY_HEADER[j] = to_fill
    j += 1
    
  endfor

  PRIMARY_HEADER[j] = "SW_VER = " + OBSERVATION.SOFTWARE_VERSION + " /SVN revision of processing code"
  PRIMARY_HEADER[j+1] = "END"
  ; END NEW PRIMARY HEADER ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

  ; Run the header through the auto-documenting system, this will attach units and descriptions
  ; based on the CSV file doc/IUVSFieldDesc.csv
  primary_header=document_image(primary_header,level=level)
  
  write_and_start_label,PRIMARY,PRIMARY_HEADER,output_path,fileinfo,start,body,silent=silent,lbl=clbl,gzip=gzip,ouf=ouf
  write_and_label,L1A_PRIMARY,'L1A_PRIMARY',ouf,fileinfo,body,silent=silent,lbl=clbl
  write_and_label,INTEGRATION,'INTEGRATION',ouf,fileinfo,body,silent=silent,lbl=clbl
  write_and_label,ENGINEERING,'ENGINEERING',ouf,fileinfo,body,silent=silent,lbl=clbl
  write_and_label,BINNING,'BINNING',ouf,fileinfo,body,silent=silent,lbl=clbl
  write_and_label,PIXELGEOMETRY,'PIXELGEOMETRY',ouf,fileinfo,body,silent=silent,lbl=clbl
  write_and_label,SPACECRAFTGEOMETRY,'SPACECRAFTGEOMETRY',ouf,fileinfo,body,silent=silent,lbl=clbl
  write_and_label,EMISSIONS,'EMISSIONS',ouf,fileinfo,body,silent=silent,lbl=clbl
  write_and_label,BRIGHTNESSES,'BRIGHTNESSES',ouf,fileinfo,body,silent=silent,lbl=clbl
  write_and_label,OBSERVATION,'OBSERVATION',ouf,fileinfo,body,silent=silent,lbl=clbl
  if keyword_set(clbl) then finish_pds4_label,start,body,ouf
  free_lun,ouf

  if FILE_TEST(output_path) eq 1 then begin
    print, "FINISHED! If you see this message when calling from Python, the IDL script succeeded"
    ; Append a success message to a log file
    get_lun, unit
    openw, unit, saveloc + 'success_log.txt', /append
    printf, unit, 'Success: ' + output_path
    close, unit
    free_lun, unit
  endif else begin
    print, "oh no problems in IDL :("
    get_lun, unit
    openw, unit, saveloc + 'failure_log.txt', /append
    printf, unit, 'Failed: ' + l1a_filepath
    close, unit
    free_lun, unit
  endelse
  
  
  
end